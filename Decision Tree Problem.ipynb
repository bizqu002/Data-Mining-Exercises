{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Consider the following set of training examples\n",
      "\n",
      "    X  Y  Z  No. of Class C1 Examples  No. of Class C2 Examples\n",
      "0  0  0  0                         5                        40\n",
      "1  0  0  1                         0                        15\n",
      "2  0  1  0                        10                         5\n",
      "3  0  1  1                        45                         0\n",
      "4  1  0  0                        10                         5\n",
      "5  1  0  1                        25                         0\n",
      "6  1  1  0                         5                        20\n",
      "7  1  1  1                         0                        15 \n",
      "\n",
      "(a) Compute a two-level decision tree using the greedy approach described in this chapter. Use the classification error rate as the criterion for splitting. What is the overall error rate of the induced tree?\n",
      "\n",
      "Error rate using attribute X : 0.5\n",
      "Error rate using attribute Y : 0.4\n",
      "Error rate using attribute Z : 0.3\n",
      "\n",
      "Z has a lower error rate. Split on attribute Z:\n",
      "\n",
      "Error rate on split Z  = 0\n",
      "Using attribute X : 0.3000\n",
      "Using attribute Y : 0.3000\n",
      "Error rate on split Z  = 1\n",
      "Using attribute X : 0.3000\n",
      "Using attribute Y : 0.3000\n",
      "\n",
      "Overall error rate : 0.3000\n",
      "\n",
      "We have the corresponding two-level decision tree:\n",
      "[inorder] C2, X or Y, C2, Z, C1, X or Y, C1\n"
     ]
    }
   ],
   "source": [
    "###Introduction to Data Mining by Tan, Steinbach & Kumar\n",
    "###Chapter 4 Problem 6 \n",
    "###Author: Brenda S. Izquierdo\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"6. Consider the following set of training examples\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'X': [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "    'Y': [0, 0, 1, 1, 0, 0, 1, 1],\n",
    "    'Z': [0, 1, 0, 1, 0, 1, 0, 1],\n",
    "    'No. of Class C1 Examples': [5, 0, 10, 45, 10, 25, 5, 0],\n",
    "    'No. of Class C2 Examples': [40, 15, 5, 0, 5, 0, 20, 15]\n",
    "})\n",
    "\n",
    "print(\"\\n\", df, \"\\n\")\n",
    "print(\"(a) Compute a two-level decision tree using the greedy approach described in this chapter. Use the classification error rate as the criterion for splitting. What is the overall error rate of the induced tree?\\n\")\n",
    "\n",
    "def error_rate(col, df):\n",
    "    sum_ = 0\n",
    "    min_ = []\n",
    "    labels = np.unique(df[col])\n",
    "    for i in range(0, len(labels)):\n",
    "        a = df.loc[(df[col] == labels[i])]['No. of Class C1 Examples'].sum()\n",
    "        b = df.loc[(df[col] == labels[i])]['No. of Class C2 Examples'].sum()\n",
    "        sum_ += a + b\n",
    "        min_.append(min(a, b))        \n",
    "    return [sum(min_), sum_]\n",
    "\n",
    "list_ = [\"X\", \"Y\", \"Z\"]\n",
    "for i in range(0, len(list_)):\n",
    "    print(\"Error rate using attribute\", list_[i], \":\", error_rate(list_[i], df)[0]/error_rate(list_[i], df)[1])\n",
    "    \n",
    "def error_rate_split(split):\n",
    "    labels = np.unique(df[split])\n",
    "    min_ = []\n",
    "    for i in range(0, len(labels)):\n",
    "        print(\"Error rate on split\", split, \" =\", labels[i])\n",
    "        min_2 = []\n",
    "        for j in range(0, len(list_)):\n",
    "            if(list_[j] != split):\n",
    "                a = error_rate(list_[j], df.loc[df[split] == labels[i]])\n",
    "                print(\"Using attribute\", list_[j], \": %.4f\" % (a[0]/a[1]))\n",
    "                min_2.append([a[0], a[1]])\n",
    "        min_.append(min(min_2))\n",
    "    print(\"\\nOverall error rate : %.4f\" % ((min_[0][0] + min_[1][0])/(min_[0][1] + min_[1][1])))\n",
    "        \n",
    "print(\"\\nZ has a lower error rate. Split on attribute Z:\\n\")\n",
    "error_rate_split(\"Z\")\n",
    "\n",
    "print(\"\\nWe have the corresponding two-level decision tree:\")\n",
    "print(\"[inorder] C2, X or Y, C2, Z, C1, X or Y, C1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b) Repeat part (a) using X as the first splitting attribute and then choose the best remaining attribute for splitting at each of the two successor nodes. What is the error rate of the induced tree? \n",
      "\n",
      "After choosing attribute X to be the first splitting attribute, the sub- sequent test condition may involve either attribute Y or attribute Z.\n",
      "\n",
      "Error rate on split X  = 0\n",
      "Using attribute Y : 0.0833\n",
      "Using attribute Z : 0.2500\n",
      "Error rate on split X  = 1\n",
      "Using attribute Y : 0.1250\n",
      "Using attribute Z : 0.3750\n",
      "\n",
      "Overall error rate : 0.1000\n",
      "\n",
      "We have the corresponding two-level decision tree:\n",
      "[inorder] C2, Y, C1, X, C1, Y, C2\n"
     ]
    }
   ],
   "source": [
    "print(\"(b) Repeat part (a) using X as the first splitting attribute and then choose the best remaining attribute for splitting at each of the two successor nodes. What is the error rate of the induced tree? \")\n",
    "print(\"\\nAfter choosing attribute X to be the first splitting attribute, the sub- sequent test condition may involve either attribute Y or attribute Z.\\n\")\n",
    "error_rate_split(\"X\")\n",
    "\n",
    "print(\"\\nWe have the corresponding two-level decision tree:\")\n",
    "print(\"[inorder] C2, Y, C1, X, C1, Y, C2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(c) Compare the results of parts (a) and (b). Comment on the suitability of the greedy heuristic used for splitting attribute selection\n",
      "From the preceding results, the error rate for part (a) is significantly larger than that for part (b). This example shows that a greedy heuristic does not always produce an optimal solution. \n"
     ]
    }
   ],
   "source": [
    "print(\"(c) Compare the results of parts (a) and (b). Comment on the suitability of the greedy heuristic used for splitting attribute selection\")\n",
    "print(\"From the preceding results, the error rate for part (a) is significantly larger than that for part (b). This example shows that a greedy heuristic does not always produce an optimal solution. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
